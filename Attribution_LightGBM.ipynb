{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhifJ6O2bWSeqlUMY93poV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/advertising/blob/main/Attribution_LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section-by-section** explanation of the code. The code performs the following main tasks:\n",
        "\n",
        "1. **Generates synthetic multi-touch attribution data**,\n",
        "2. **Transforms it into user-level features**,\n",
        "3. **Trains a LightGBM model** to predict conversion, and\n",
        "4. **Analyzes model results** (feature importance and channel-level attribution).\n",
        "\n",
        "---\n",
        "\n",
        "## Imports\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "```\n",
        "\n",
        "1. **NumPy (`numpy`)**: Provides support for large, multi-dimensional arrays and random number generation.  \n",
        "2. **Pandas (`pandas`)**: Offers data manipulation and analysis tools, particularly with `DataFrame`s.  \n",
        "3. **Datetime, Timedelta**: Python’s built-in library for handling dates, times, and time spans.  \n",
        "4. **LightGBM (`lightgbm`)**: A gradient boosting framework that is particularly efficient for machine learning tasks, especially on large datasets.  \n",
        "5. **Scikit-learn** components:  \n",
        "   - `train_test_split`: Splits arrays or DataFrames into random train and test subsets.  \n",
        "   - `roc_auc_score`: Calculates the Area Under the Receiver Operating Characteristic Curve—a common metric for binary classification.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# Optional: Fix random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "```\n",
        "- **Sets the random seed** to ensure that all random processes (e.g., NumPy random operations) produce the same results each time the script is run. This is helpful for reproducible experiments.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Generate a Single Synthetic Journey\n",
        "\n",
        "```python\n",
        "def generate_synthetic_journey():\n",
        "    \"\"\"\n",
        "    Generate a single synthetic customer journey\n",
        "    \"\"\"\n",
        "    channels = ['search', 'social', 'email', 'display', 'organic']\n",
        "    \n",
        "    # Random number of touchpoints (1-8)\n",
        "    num_touchpoints = np.random.randint(1, 9)\n",
        "    \n",
        "    journey = []\n",
        "    timestamps = []\n",
        "    base_time = datetime.now()\n",
        "    \n",
        "    for i in range(num_touchpoints):\n",
        "        if i == 0:\n",
        "            # First touch more likely to be search or social\n",
        "            channel = np.random.choice(['search', 'social', 'organic'],\n",
        "                                       p=[0.4, 0.4, 0.2])\n",
        "        elif i == num_touchpoints - 1:\n",
        "            # Last touch more likely to be email or search\n",
        "            channel = np.random.choice(['email', 'search', 'social'],\n",
        "                                       p=[0.4, 0.4, 0.2])\n",
        "        else:\n",
        "            # Mid-journey touches\n",
        "            channel = np.random.choice(channels,\n",
        "                                       p=[0.3, 0.2, 0.2, 0.2, 0.1])\n",
        "        \n",
        "        # Add randomness to timestamps (between 1-72 hours)\n",
        "        time_delta = timedelta(hours=np.random.randint(1, 72))\n",
        "        timestamp = base_time + time_delta\n",
        "        base_time = timestamp\n",
        "        \n",
        "        journey.append(channel)\n",
        "        timestamps.append(timestamp)\n",
        "    \n",
        "    # Generate conversion probability\n",
        "    has_email = 'email' in journey\n",
        "    has_search = 'search' in journey\n",
        "    base_conv_prob = 0.3\n",
        "    \n",
        "    if has_email and has_search:\n",
        "        conv_prob = base_conv_prob * 1.5\n",
        "    elif has_email or has_search:\n",
        "        conv_prob = base_conv_prob * 1.2\n",
        "    else:\n",
        "        conv_prob = base_conv_prob\n",
        "    \n",
        "    converted = np.random.random() < conv_prob\n",
        "    \n",
        "    return journey, timestamps, converted\n",
        "```\n",
        "\n",
        "**Function Purpose**: Creates a **single user’s journey** through various marketing channels.\n",
        "\n",
        "1. **Define possible channels**: `channels = ['search', 'social', 'email', 'display', 'organic']`.\n",
        "2. **Pick a random number of touchpoints** (1 to 8).\n",
        "3. **Initialize**:\n",
        "   - `journey` as an empty list (will store channels).\n",
        "   - `timestamps` as an empty list (will store times for each channel interaction).\n",
        "   - `base_time` as `datetime.now()` (a starting timestamp).\n",
        "4. **Loop over each touchpoint** and pick the channel:\n",
        "   - For the **first touch** (`i == 0`), favor “search” or “social” (some probability distribution).\n",
        "   - For the **last touch** (`i == num_touchpoints - 1`), favor “email” or “search.”\n",
        "   - For **intermediate touches**, randomly pick any channel using the probabilities `[0.3, 0.2, 0.2, 0.2, 0.1]`.\n",
        "5. **Assign a random offset** (`time_delta`) of 1–72 hours from the current `base_time`, update `base_time` to that new timestamp, and store it in `timestamps`.\n",
        "6. **Calculate conversion**:  \n",
        "   - Base probability is 0.3.  \n",
        "   - If the user’s journey included **email and search**, multiply by 1.5.  \n",
        "   - If the user’s journey included **email or search**, multiply by 1.2.  \n",
        "   - Draw a random number `converted = np.random.random() < conv_prob`.\n",
        "7. **Return** the resulting journey (list of channels), timestamps, and whether they converted (boolean).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Generate a Complete Dataset\n",
        "\n",
        "```python\n",
        "def generate_dataset(num_users=10000):\n",
        "    \"\"\"\n",
        "    Generate a complete dataset of user journeys\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    \n",
        "    for user_id in range(num_users):\n",
        "        journey, timestamps, converted = generate_synthetic_journey()\n",
        "        \n",
        "        for i in range(len(journey)):\n",
        "            data.append({\n",
        "                'user_id': user_id,\n",
        "                'channel': journey[i],\n",
        "                'timestamp': timestamps[i].strftime('%Y-%m-%d %H:%M:%S'),  # Convert to string\n",
        "                'touch_point': i + 1,\n",
        "                'journey_length': len(journey),\n",
        "                'converted': converted\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    # Convert timestamp back to datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    return df\n",
        "```\n",
        "\n",
        "**Function Purpose**: Uses the single-journey generator (`generate_synthetic_journey`) to create a **dataset** for multiple users.\n",
        "\n",
        "1. `num_users=10000`: By default, generates journeys for 10,000 users.\n",
        "2. Initialize `data` as a list to store row-by-row records.\n",
        "3. For each user:\n",
        "   - Call `generate_synthetic_journey()` to get their `journey`, `timestamps`, and `converted` status.\n",
        "   - Loop through each touchpoint in that journey, appending a dictionary with:\n",
        "     - `user_id`\n",
        "     - `channel` (the channel at this touch)\n",
        "     - `timestamp` (converted to string for convenience)\n",
        "     - `touch_point` (the order in the journey)\n",
        "     - `journey_length` (total number of touchpoints for this user)\n",
        "     - `converted` (did they convert or not)\n",
        "4. Create a Pandas `DataFrame` (`df`) from `data`.\n",
        "5. Convert the `timestamp` back to a `datetime` object using `pd.to_datetime`.\n",
        "6. Return the resulting DataFrame, which has one row per touchpoint.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Create Features for the Model\n",
        "\n",
        "```python\n",
        "def create_features(df):\n",
        "    \"\"\"\n",
        "    Transform raw journey data into features for LightGBM\n",
        "    \"\"\"\n",
        "    grouped = df.groupby('user_id')\n",
        "\n",
        "    # Initialize user-level feature DataFrame\n",
        "    user_features = pd.DataFrame(index=grouped.groups.keys())\n",
        "    \n",
        "    # Basic journey features\n",
        "    user_features['journey_length'] = grouped['journey_length'].first()\n",
        "    \n",
        "    # Calculate time duration in hours (timestamp max - timestamp min)\n",
        "    user_features['total_time'] = grouped['timestamp'].agg(\n",
        "        lambda x: (x.max() - x.min()).total_seconds() / 3600\n",
        "    )\n",
        "    \n",
        "    # Channel-specific features\n",
        "    channels = ['search', 'social', 'email', 'display', 'organic']\n",
        "    \n",
        "    # Collect channel sequences (lists) for each user once\n",
        "    channel_data = grouped['channel'].agg(list)\n",
        "    \n",
        "    for channel in channels:\n",
        "        # Count occurrences\n",
        "        user_features[f'{channel}_count'] = channel_data.apply(lambda x: x.count(channel))\n",
        "        \n",
        "        # Calculate frequency\n",
        "        user_features[f'{channel}_freq'] = (\n",
        "            user_features[f'{channel}_count'] / user_features['journey_length']\n",
        "        )\n",
        "        \n",
        "        # First touch\n",
        "        user_features[f'{channel}_first'] = channel_data.apply(lambda x: 1 if x[0] == channel else 0)\n",
        "        \n",
        "        # Last touch\n",
        "        user_features[f'{channel}_last'] = channel_data.apply(lambda x: 1 if x[-1] == channel else 0)\n",
        "    \n",
        "    # Add conversion target\n",
        "    user_features['converted'] = grouped['converted'].first()\n",
        "    \n",
        "    return user_features\n",
        "```\n",
        "\n",
        "**Function Purpose**: Aggregates **touchpoint-level** data to **user-level** data and creates modeling features.\n",
        "\n",
        "1. **Group by `user_id`**: `grouped = df.groupby('user_id')`.\n",
        "2. **Initialize** an empty `DataFrame` called `user_features`, using the unique user IDs as its index.\n",
        "3. **Journey Length**:\n",
        "   - For each user, retrieve the first `journey_length` value from the group and store it in `user_features['journey_length']`.\n",
        "4. **Total Time**:\n",
        "   - Compute how long the journey took in hours: `(x.max() - x.min()).total_seconds() / 3600`.\n",
        "5. **Channel-Specific Features**:\n",
        "   - Create a list of all channels each user encountered: `channel_data = grouped['channel'].agg(list)`.\n",
        "   - For each possible channel (search, social, email, display, organic):\n",
        "     - `channel_count`: How many times that channel appears in the user’s journey.  \n",
        "     - `channel_freq`: The above count divided by the total journey length for that user.  \n",
        "     - `channel_first`: 1 if the first touch channel is the given channel, else 0.  \n",
        "     - `channel_last`: 1 if the last touch channel is the given channel, else 0.\n",
        "6. **Conversion Target**:\n",
        "   - `converted` is just the first conversion flag in each user’s group (they are all the same for a given user).\n",
        "7. **Return** the resulting `DataFrame` with one row per user and columns representing features and the conversion label.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Train a LightGBM Model with Callbacks\n",
        "\n",
        "```python\n",
        "def train_model(features, target):\n",
        "    \"\"\"\n",
        "    Train LightGBM model with the synthetic data using callbacks\n",
        "    for early stopping instead of the early_stopping_rounds parameter.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'max_depth': 5,\n",
        "        'learning_rate': 0.1,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
        "    \n",
        "    # Use callbacks for early stopping\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=10),\n",
        "            lgb.log_evaluation(period=10)  # Set period=0 or any other value as needed\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return model, X_test, y_test\n",
        "```\n",
        "\n",
        "**Function Purpose**: Trains a **LightGBM** model on the features created above.\n",
        "\n",
        "1. **Parameters (`params`)**:  \n",
        "   - `objective='binary'`: Binary classification.  \n",
        "   - `metric='auc'`: Use AUC (Area Under the ROC Curve) as the metric.  \n",
        "   - `boosting_type='gbdt'`: Use traditional Gradient Boosting Decision Trees.  \n",
        "   - `num_leaves=31`, `max_depth=5`: Control complexity of trees.  \n",
        "   - `learning_rate=0.1`: Step size shrinkage.  \n",
        "   - `feature_fraction=0.9`, `bagging_fraction=0.8`, `bagging_freq=5`: Subsampling features/rows for regularization.  \n",
        "   - `verbose=-1`: Suppress detailed logging.\n",
        "2. **Split the Data**:  \n",
        "   - Use `train_test_split` to create an 80/20 train/test split.\n",
        "3. **Create LightGBM Datasets**:  \n",
        "   - `train_data` = `lgb.Dataset(X_train, label=y_train)`.  \n",
        "   - `valid_data` = `lgb.Dataset(X_test, label=y_test)`.\n",
        "4. **Train with Callbacks**:  \n",
        "   - `lgb.early_stopping(stopping_rounds=10)` stops training if the validation metric does not improve for 10 consecutive rounds.  \n",
        "   - `lgb.log_evaluation(period=10)` logs evaluation results every 10 rounds.\n",
        "5. **Return** the trained `model` and the test split (`X_test`, `y_test`) for later evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Analyze Results\n",
        "\n",
        "```python\n",
        "def analyze_results(model, feature_names, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Analyze and print model results\n",
        "    \"\"\"\n",
        "    # Feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importance(importance_type='gain')\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Channel attribution\n",
        "    channels = ['search', 'social', 'email', 'display', 'organic']\n",
        "    channel_importance = {}\n",
        "    \n",
        "    for channel in channels:\n",
        "        # Sum importance of features that contain the channel name\n",
        "        channel_feat_df = importance_df[importance_df['feature'].str.contains(channel)]\n",
        "        channel_importance[channel] = channel_feat_df['importance'].sum()\n",
        "    \n",
        "    # Normalize to percentages\n",
        "    total_importance = sum(channel_importance.values())\n",
        "    if total_importance > 0:\n",
        "        channel_importance = {\n",
        "            k: (v / total_importance) * 100 for k, v in channel_importance.items()\n",
        "        }\n",
        "    else:\n",
        "        channel_importance = {k: 0 for k in channels}\n",
        "    \n",
        "    # Model performance (AUC)\n",
        "    y_pred = model.predict(X_test)\n",
        "    auc_score = roc_auc_score(y_test, y_pred)\n",
        "    \n",
        "    return importance_df, channel_importance, auc_score\n",
        "```\n",
        "\n",
        "**Function Purpose**: Evaluates the **LightGBM model** and helps with **channel attribution**.\n",
        "\n",
        "1. **Feature Importance**:  \n",
        "   - `model.feature_importance(importance_type='gain')` returns how much each feature contributed to reducing loss (the total gain).  \n",
        "   - Create a `DataFrame` mapping `feature` name to its `importance`.  \n",
        "   - Sort by descending importance.\n",
        "2. **Channel Attribution**:\n",
        "   - We have multiple features for each channel (e.g., `search_count`, `search_freq`, `search_first`, `search_last`).  \n",
        "   - The code filters the `importance_df` for rows whose `feature` contains the channel’s name (like “search”).  \n",
        "   - Sums the importance for those rows.  \n",
        "   - Normalizes the sums so that the total across all channels is 100%.\n",
        "3. **Calculate AUC**:\n",
        "   - Predict on the test set: `y_pred = model.predict(X_test)`.  \n",
        "   - Compare predictions vs. true labels using `roc_auc_score(y_test, y_pred)`.\n",
        "4. **Return**:\n",
        "   - `importance_df` (all features and their importance scores),  \n",
        "   - `channel_importance` (aggregated feature importances per channel),  \n",
        "   - `auc_score` (model’s performance metric).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Main Script\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate synthetic data\n",
        "    print(\"Generating synthetic data...\")\n",
        "    raw_data = generate_dataset(num_users=10000)\n",
        "    print(f\"Generated {len(raw_data)} touchpoints\")\n",
        "    \n",
        "    # Create features\n",
        "    print(\"Creating features...\")\n",
        "    features_df = create_features(raw_data)\n",
        "    print(f\"Created features for {len(features_df)} users\")\n",
        "    \n",
        "    # Split features and target\n",
        "    X = features_df.drop('converted', axis=1)\n",
        "    y = features_df['converted']\n",
        "    \n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    model, X_test, y_test = train_model(X, y)\n",
        "    \n",
        "    # Analyze results\n",
        "    print(\"\\nAnalyzing results...\")\n",
        "    importance, channel_importance, auc_score = analyze_results(\n",
        "        model, X.columns, X_test, y_test\n",
        "    )\n",
        "    \n",
        "    print(\"\\nChannel Attribution Scores:\")\n",
        "    for channel, score in sorted(channel_importance.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{channel}: {score:.2f}%\")\n",
        "    \n",
        "    print(f\"\\nModel AUC Score: {auc_score:.3f}\")\n",
        "    \n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(importance.head(10))\n",
        "```\n",
        "\n",
        "1. **`if __name__ == \"__main__\":`**: Pythonic entry point to the script.\n",
        "2. **Generate Synthetic Data**:\n",
        "   - `generate_dataset(num_users=10000)` creates a large DataFrame with each touchpoint.  \n",
        "   - Print how many rows (touchpoints) were created.\n",
        "3. **Create Features**:\n",
        "   - `create_features(raw_data)` aggregates the touchpoint-level DataFrame to user-level features.  \n",
        "   - Print how many users it created features for.\n",
        "4. **Prepare Data for Model**:\n",
        "   - `X` = all columns except `'converted'`.  \n",
        "   - `y` = the `'converted'` column.\n",
        "5. **Train Model**:\n",
        "   - `train_model(X, y)` returns the trained `model`, plus the split test data.  \n",
        "   - Print status updates.\n",
        "6. **Analyze Results**:\n",
        "   - Pass `model`, the columns of `X`, and the test splits to `analyze_results`.  \n",
        "   - **Outputs**:\n",
        "     - **Feature importance** table (`importance`).\n",
        "     - **Channel-level scores** (`channel_importance`).\n",
        "     - **AUC score** (`auc_score`).\n",
        "7. **Print Results**:\n",
        "   - **Channel Attribution Scores**: Sort channels by their total importance share.  \n",
        "   - **AUC Score**: Evaluate the classification performance.  \n",
        "   - **Top 10 Most Important Features**: Show which features were most important.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Overall Flow**:  \n",
        "  1. **Synthetic Data Generation** (user journeys) ->\n",
        "  2. **Feature Engineering** (aggregate user-level data) ->\n",
        "  3. **Train Model** (LightGBM) ->\n",
        "  4. **Evaluate & Analyze** (feature importance, AUC, channel attribution).\n",
        "\n",
        "- **Key Concepts**:\n",
        "  - **Synthetic Data**: We randomly create journeys rather than using real marketing data.  \n",
        "  - **Attribution**: The model identifies which channels/features are most responsible for driving conversions.  \n",
        "  - **LightGBM**: A fast, efficient boosting library to handle large datasets.  \n",
        "  - **Callback-based Early Stopping**: We use `lgb.early_stopping(stopping_rounds=10)` instead of the older `early_stopping_rounds` parameter for compatibility.  \n",
        "\n",
        "You can run this entire script to generate data, train a model, and see which channels and features are most important for conversion in this synthetic scenario."
      ],
      "metadata": {
        "id": "xyB4RhscCP4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Optional: Fix random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_synthetic_journey():\n",
        "    \"\"\"\n",
        "    Generate a single synthetic customer journey\n",
        "    \"\"\"\n",
        "    channels = ['search', 'social', 'email', 'display', 'organic']\n",
        "\n",
        "    # Random number of touchpoints (1-8)\n",
        "    num_touchpoints = np.random.randint(1, 9)\n",
        "\n",
        "    journey = []\n",
        "    timestamps = []\n",
        "    base_time = datetime.now()\n",
        "\n",
        "    for i in range(num_touchpoints):\n",
        "        if i == 0:\n",
        "            # First touch more likely to be search or social\n",
        "            channel = np.random.choice(['search', 'social', 'organic'],\n",
        "                                       p=[0.4, 0.4, 0.2])\n",
        "        elif i == num_touchpoints - 1:\n",
        "            # Last touch more likely to be email or search\n",
        "            channel = np.random.choice(['email', 'search', 'social'],\n",
        "                                       p=[0.4, 0.4, 0.2])\n",
        "        else:\n",
        "            # Mid-journey touches\n",
        "            channel = np.random.choice(channels,\n",
        "                                       p=[0.3, 0.2, 0.2, 0.2, 0.1])\n",
        "\n",
        "        # Add randomness to timestamps (between 1-72 hours)\n",
        "        time_delta = timedelta(hours=np.random.randint(1, 72))\n",
        "        timestamp = base_time + time_delta\n",
        "        base_time = timestamp\n",
        "\n",
        "        journey.append(channel)\n",
        "        timestamps.append(timestamp)\n",
        "\n",
        "    # Generate conversion probability\n",
        "    has_email = 'email' in journey\n",
        "    has_search = 'search' in journey\n",
        "    base_conv_prob = 0.3\n",
        "\n",
        "    if has_email and has_search:\n",
        "        conv_prob = base_conv_prob * 1.5\n",
        "    elif has_email or has_search:\n",
        "        conv_prob = base_conv_prob * 1.2\n",
        "    else:\n",
        "        conv_prob = base_conv_prob\n",
        "\n",
        "    converted = np.random.random() < conv_prob\n",
        "\n",
        "    return journey, timestamps, converted\n",
        "\n",
        "def generate_dataset(num_users=10000):\n",
        "    \"\"\"\n",
        "    Generate a complete dataset of user journeys\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    for user_id in range(num_users):\n",
        "        journey, timestamps, converted = generate_synthetic_journey()\n",
        "\n",
        "        for i in range(len(journey)):\n",
        "            data.append({\n",
        "                'user_id': user_id,\n",
        "                'channel': journey[i],\n",
        "                'timestamp': timestamps[i].strftime('%Y-%m-%d %H:%M:%S'),  # Convert to string\n",
        "                'touch_point': i + 1,\n",
        "                'journey_length': len(journey),\n",
        "                'converted': converted\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    # Convert timestamp back to datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    return df\n",
        "\n",
        "def create_features(df):\n",
        "    \"\"\"\n",
        "    Transform raw journey data into features for LightGBM\n",
        "    \"\"\"\n",
        "    grouped = df.groupby('user_id')\n",
        "\n",
        "    # Initialize user-level feature DataFrame\n",
        "    user_features = pd.DataFrame(index=grouped.groups.keys())\n",
        "\n",
        "    # Basic journey features\n",
        "    user_features['journey_length'] = grouped['journey_length'].first()\n",
        "\n",
        "    # Calculate time duration in hours (timestamp max - timestamp min)\n",
        "    user_features['total_time'] = grouped['timestamp'].agg(\n",
        "        lambda x: (x.max() - x.min()).total_seconds() / 3600\n",
        "    )\n",
        "\n",
        "    # Channel-specific features\n",
        "    channels = ['search', 'social', 'email', 'display', 'organic']\n",
        "\n",
        "    # Collect channel sequences (lists) for each user once\n",
        "    channel_data = grouped['channel'].agg(list)\n",
        "\n",
        "    for channel in channels:\n",
        "        # Count occurrences\n",
        "        user_features[f'{channel}_count'] = channel_data.apply(lambda x: x.count(channel))\n",
        "\n",
        "        # Calculate frequency\n",
        "        user_features[f'{channel}_freq'] = (\n",
        "            user_features[f'{channel}_count'] / user_features['journey_length']\n",
        "        )\n",
        "\n",
        "        # First touch\n",
        "        user_features[f'{channel}_first'] = channel_data.apply(lambda x: 1 if x[0] == channel else 0)\n",
        "\n",
        "        # Last touch\n",
        "        user_features[f'{channel}_last'] = channel_data.apply(lambda x: 1 if x[-1] == channel else 0)\n",
        "\n",
        "    # Add conversion target\n",
        "    user_features['converted'] = grouped['converted'].first()\n",
        "\n",
        "    return user_features\n",
        "\n",
        "def train_model(features, target):\n",
        "    \"\"\"\n",
        "    Train LightGBM model with the synthetic data using callbacks\n",
        "    for early stopping instead of the early_stopping_rounds parameter.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'max_depth': 5,\n",
        "        'learning_rate': 0.1,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
        "\n",
        "    # Use callbacks for early stopping\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=10),\n",
        "            lgb.log_evaluation(period=10)  # Set period=0 or any other value as needed\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model, X_test, y_test\n",
        "\n",
        "def analyze_results(model, feature_names, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Analyze and print model results\n",
        "    \"\"\"\n",
        "    # Feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importance(importance_type='gain')\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Channel attribution\n",
        "    channels = ['search', 'social', 'email', 'display', 'organic']\n",
        "    channel_importance = {}\n",
        "\n",
        "    for channel in channels:\n",
        "        # Sum importance of features that contain the channel name\n",
        "        channel_feat_df = importance_df[importance_df['feature'].str.contains(channel)]\n",
        "        channel_importance[channel] = channel_feat_df['importance'].sum()\n",
        "\n",
        "    # Normalize to percentages\n",
        "    total_importance = sum(channel_importance.values())\n",
        "    if total_importance > 0:\n",
        "        channel_importance = {\n",
        "            k: (v / total_importance) * 100 for k, v in channel_importance.items()\n",
        "        }\n",
        "    else:\n",
        "        channel_importance = {k: 0 for k in channels}\n",
        "\n",
        "    # Model performance (AUC)\n",
        "    y_pred = model.predict(X_test)\n",
        "    auc_score = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    return importance_df, channel_importance, auc_score\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate synthetic data\n",
        "    print(\"Generating synthetic data...\")\n",
        "    raw_data = generate_dataset(num_users=10000)\n",
        "    print(f\"Generated {len(raw_data)} touchpoints\")\n",
        "\n",
        "    # Create features\n",
        "    print(\"Creating features...\")\n",
        "    features_df = create_features(raw_data)\n",
        "    print(f\"Created features for {len(features_df)} users\")\n",
        "\n",
        "    # Split features and target\n",
        "    X = features_df.drop('converted', axis=1)\n",
        "    y = features_df['converted']\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    model, X_test, y_test = train_model(X, y)\n",
        "\n",
        "    # Analyze results\n",
        "    print(\"\\nAnalyzing results...\")\n",
        "    importance, channel_importance, auc_score = analyze_results(\n",
        "        model, X.columns, X_test, y_test\n",
        "    )\n",
        "\n",
        "    print(\"\\nChannel Attribution Scores:\")\n",
        "    for channel, score in sorted(channel_importance.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{channel}: {score:.2f}%\")\n",
        "\n",
        "    print(f\"\\nModel AUC Score: {auc_score:.3f}\")\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(importance.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvBWBdTPEZgT",
        "outputId": "1b741f09-07e4-4cb1-9f31-34807fb03df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic data...\n",
            "Generated 45127 touchpoints\n",
            "Creating features...\n",
            "Created features for 10000 users\n",
            "Training model...\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[10]\ttrain's auc: 0.612341\tvalid's auc: 0.536423\n",
            "[20]\ttrain's auc: 0.62324\tvalid's auc: 0.544061\n",
            "Early stopping, best iteration is:\n",
            "[15]\ttrain's auc: 0.617702\tvalid's auc: 0.551053\n",
            "\n",
            "Analyzing results...\n",
            "\n",
            "Channel Attribution Scores:\n",
            "email: 34.88%\n",
            "search: 32.39%\n",
            "social: 13.76%\n",
            "display: 12.00%\n",
            "organic: 6.98%\n",
            "\n",
            "Model AUC Score: 0.551\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "           feature  importance\n",
            "1       total_time  462.245380\n",
            "3      search_freq  236.422332\n",
            "10     email_count  200.526920\n",
            "11      email_freq  112.401075\n",
            "7      social_freq   72.516461\n",
            "0   journey_length   65.500460\n",
            "15    display_freq   57.750719\n",
            "14   display_count   52.423113\n",
            "19    organic_freq   51.304351\n",
            "2     search_count   51.230919\n"
          ]
        }
      ]
    }
  ]
}